{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SSB57306 Advanced Omics: Computational Metagenomics Tutorial","text":"<p>This tutorial teaches state-of-the-art computational methods for the analysis of metagenome data, and will focus on foundational knowledge and skills in experimental design, sequencing technologies, quality control, assembly and binning.</p> <p>Software packages used are freeware.</p>"},{"location":"data-processing/assembly/","title":"Assembly","text":"<p>Learning objectives</p> <ul> <li>Being able to create an assembly with megahit</li> <li>Assess the quality of the assembly</li> </ul> <p>Now that our reads are quality trimmed and ready to go is time to start the assembly. We could use megahit as follows, but we will not run this during the tutorial:</p> <pre><code>megahit --12 sample_0.nophix.fastq.gz,sample_1.nophix.fastq.gz,sample_2.nophix.fastq.gz,sample_3.nophix.fastq.gz,sample_4.nophix.fastq.gz,sample_5.nophix.fastq.gz \\\n        -t 16 \\\n        -o megahit_assembly_meta \\\n        --presets meta-sensitive\n</code></pre> <p>This command would take around 50 minutes to complete, to speed up things we pre-assembled the data which is available in the precomputed/assembly/ folder. </p> <p>Feel free to inspect the contents of the folder by using</p> <pre><code>ls -lh /data/precomputed/assembly/\n</code></pre> <p>You will notice the final.contigs.fa file which contains the assembly</p> <p>Now, we want to find out how well or poorly our assembly went. For this, we use quast, a tool to generate an assembly report.</p> <pre><code>quast /data/precomputed/assembly/final.contigs.fa -o quast/\n</code></pre> <p>Then, we inspect the output from quast. </p> <pre><code>less quast/report.txt\n</code></pre> <p>Not bad! We know our metagenome is not too large and if we care about contiguity, contigs above 5kb represent most of our community</p>"},{"location":"data-processing/binning/","title":"Binning","text":"<p>Learning objectives</p> <ul> <li>Extract individual genomes from an assembly</li> </ul> <p>Next, we would want to map the reads back on the assembly to see how the coverage for each contig changed across samples such as below. Here, we won\u2019t run it for all samples to save time and space.</p> <pre><code>bwa mem /data/precomputed/assembly/final.contigs.fa \\\n        reads/sample_0.fq.gz \\\n        -o bam/sample_0.bam \\\n        -t 32\n</code></pre> <p>You will find the bam files in the precomputed/bam/ folder. </p> <p>Question</p> <p>Feel free to inspect the content as done before, do you notice something particular?</p> <p>Now, many tools need bam files to be sorted in order to work. Therefore, we will use samtools sort to do that.</p> <pre><code>mkdir bam\nsamtools sort /data/precomputed/bam/sample_0.bam -o bam/sample_0.sorted.bam\n</code></pre> <p>The sorted bam files can also be index, so other tools can quickly extract alignments</p> <pre><code>for bam in bam/*.sorted.bam; do samtools index $bam; done\n</code></pre> <p>We can first get an idea if the different genomes can be separated by gc-content and coverage. With Blobtools the coverage and gc-content of the contigs can be plotted and visualy bins or blobs can be detected</p> <pre><code>wget ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump.tar.gz\ntar zxvf taxdump.tar.gz\n\nblobtools create -i /data/precomputed/assembly/final.contigs.fa -b bam/sample_0.sorted.bam -b /data/precomputed/bam/sample_1.sorted.bam -b /data/precomputed/bam/sample_2.sorted.bam -b /data/precomputed/bam/sample_3.sorted.bam -b /data/precomputed/bam/sample_4.sorted.bam -b /data/precomputed/bam/sample_5.sorted.bam --nodes nodes.dmp --names names.dmp --db nodesDB.txt\n\nblobtools view -i blobDB.json\nblobtools plot -i blobDB.json\n</code></pre> <p>Question</p> <p>Take a look at the <code>blobDB.json.bestsum.phylum.p8.span.100.blobplot.bam0.png</code> file. How many large genome bins can you detect? How is this for the other files?</p> <p>Now, we will create a depth table, which can be used by binning tools to identify genomic entities (contigs, here) that have similar coverage across samples.</p> <pre><code>jgi_summarize_bam_contig_depths --outputDepth depth.tsv /data/precomputed/bam/sample_0.sorted.bam /data/precomputed/bam/sample_1.sorted.bam /data/precomputed/bam/sample_2.sorted.bam /data/precomputed/bam/sample_3.sorted.bam /data/precomputed/bam/sample_4.sorted.bam /data/precomputed/bam/sample_5.sorted.bam\n</code></pre> <p>Now, we can run metabat to find our bins:</p> <pre><code>metabat2 -a depth.tsv -i /data/precomputed/assembly/final.contigs.fa.gz -o binned_genomes/bin\n</code></pre> <p>After this is done, we will now try and figure out how good are our bins, we will use checkm. First we create a set of lineage specific markers for bacterial genomes:</p> <pre><code>checkm taxon_set domain Bacteria checkm_taxon_bacteria\n</code></pre> <p>Now, it\u2019s time to find out the completeness of your binned genomes:</p> <pre><code>checkm analyze -x fa checkm_taxon_bacteria binned_genomes/ checkm_bac/\n</code></pre> <p>This created the checkm_bac/ folder with the information on our bins, to interpret the results we use checkm qa:</p> <pre><code>checkm qa checkm_taxon_bacteria checkm_bac/\n</code></pre> <p>Question</p> <p>What does it mean? How can we interpret this?</p> <p>Now, we will let checkm predict the taxonomy of the bins and evaluate their completeness.</p> <pre><code>checkm lineage_wf -t 8 -x fa binned_genomes/ checkm_taxonomy/\n</code></pre> <p>This command will take some time but it will give us a detailed breakdown of each genome predicted taxonomy and completeness.</p> <p>With the bam files, a coverage file can also be created with the tool CoverM</p> <pre><code>coverm contig -b /data/precomputed/bam/sample_*.sorted.bam -m mean -t 16 -o coverage.tsv\n</code></pre>"},{"location":"data-processing/intro/","title":"Introduction","text":""},{"location":"data-processing/intro/#dataset-explanation","title":"Dataset explanation","text":"<ul> <li>In the practical you will go from raw reads to taxonomical characterization of metagenome assembled genomes.</li> <li>In this mock experiment we have a small diverse synthetic rhizosphere community.</li> </ul>"},{"location":"data-processing/intro/#jupyter-hub","title":"Jupyter Hub","text":"<p>In this course we use a jupyterhub setup to provide access to a linux environment, and python and R notebooks. Students and instructors have individual accounts and can log in to the jupyterhub.</p>"},{"location":"data-processing/intro/#logging-in","title":"Logging in","text":"<p>Every participant has an individual account with username/password based on first name and last name following a fixed scheme. </p> <p>Username is first_name.last_name and password is pw4first_name, all in lowercase. Any spaces an dashes in names are represented with low dashes.</p> <p>Examples:</p> <ul> <li>rens.holmer, pw4rens</li> <li>marnix.medema, pw4marnix</li> </ul> <p>In theory this makes it easy to find out another person\u2019s credentials, we trust the participants to not take advantage of this.</p>"},{"location":"data-processing/intro/#basic-jupyterhub-environment","title":"Basic jupyterhub environment","text":"<p>After logging in, an individual session will be started and you will get access to a launch screen from where you can start linux terminals, and R and python notebooks (this might look familiar if you know jupyter lab). In addition, there is a file browser that you can use to browse your home directory. If you first start your session your home is empty and there will be nothing to browse.</p>"},{"location":"data-processing/intro/#linux-terminal","title":"Linux terminal","text":"<p>A new terminal session starts in your home folder. Files in your home folder can be browsed with the file browser on the left of the screen. A conda environment with all bioinformatics software requirements is available.  Data, databases and precomputed results are available in /data.</p>"},{"location":"data-processing/quality-control/","title":"Quality Control","text":"<p>Learning objectives</p> <ul> <li>Understanding the FastQ format</li> <li>Interpret FastQC reports</li> <li>Create high quality reads by trimmign and filtering with FastP and BBduk</li> </ul>"},{"location":"data-processing/quality-control/#fastq-format","title":"FastQ format","text":"<p>First let's take a look at the data</p> <pre><code>zcat /data/reads/sample_0.fq.gz | head\n</code></pre> The output looks like this <pre><code>@gi|1184849861|gb|KY629563.1|-34525/1\nCTGATCAACGTGGGAATGAACATGGAGCAGAGCGGCCAGTGTCTCAACCTGTGCTCCGCCTTCATGCACTCTCGCGGCGCCTTCAAGCCCGGCGACATCGACGTGGAAATTTTCACCATGCCCACCAAGTTCGGGCGCGTCAGCACCACG\n+\nDDDGGEG*DIIGH?KKHJGHGKKKJKKJJ9JDK=JKFKKGJHIKHEIJJJKKGIHKGDE1DKA&gt;KEGBEEEBGI?BBEGHEEEFEEE9ED;FCB3BEEDCBEE5EEC'CA$E9EEEAEEEFEE?FFCEEE;$EE)DBCDEEDDECAAE'$\n@gi|1184849861|gb|KY629563.1|-34523/1\nGCCTTTGTCGGGTAAGGGGTGTGGCCCTCCTCCCGACAAGGCGGGCCACGGTTCGCCAGCGAACTAGGTCGGGGAGCTGGGAAGGAGCCGGAATCGGGTGGCCCCAATTTCGGGGAGAGGTTTGGGCGTCAGCCGCCCGGAAGCTCGTCG\n+\nDDDE2GGGIIIIIKKJKHJKEDJ=AIHKKHKJKKKKJHBIE$KJJJCEKJB=JH@JKEHKGEEEEDAJECDGC?EIFEBEDDF6FGEEDEE$FBDCEEA@EE$4EE$E??CDE?ED;CCDE1DEBE;ECC$?$$AEDA;EDD@$EEDE=F\n@gi|1184849861|gb|KY629563.1|-34521/1\nAGACCGAGCCCTTCCTTATAGTGGATTTCTCCGGTTCCGTCAACCAAATTTGCAGTAATGCGGGAGCGACGCTTCCACGAAATGGTCACTGTCCCGTCCACCTCGGAGAGCTTTACGCTTTCCGGTGTGTAGGGCTTGAGATCGATCGAT\n</code></pre> <p>There is a pattern that is repeated every 4 lines. This is the FASTQ format. It follows this structure:</p> Line Description 1 Always begins with '@' and then information about the read 2 The actual DNA sequence 3 Always begins with a '+' and sometimes the same info in line 1 4 Has a string of characters which represent the quality scores; must have same number of characters as line 2 <p>The fourth line shows the quality of the read. To make the sequence and the qaility align well, the numerical score is converted into a code where each individual character represents the numerical quality of an individual nucleotide, following this scheme:</p> <p>FASTQ quality encoding</p> <pre><code>    Quality encoding: !\"#$%&amp;'()*+,-./0123456789:;&lt;=&gt;?@ABCDEFGHIJ\n                    |         |         |         |         |\n    Quality score:    01........11........21........31........41\n</code></pre> <p>NextSeq and NovaSeq data often contains poly-g tails. Let's check if this also is the case for our simulated input</p> <pre><code>$ zcat /data/reads/sample_0.fq.gz | grep -E \"GGGGGGGGGGGGGGGGGG$\"\n</code></pre> <p>This search does not retun any hits. But often the end of the sequences contain stretches of poly-g. Below is an example how poly-g tails look like in Novaseq data.</p> Sequence data with poly-g tails <pre><code>GGCCAGGACCACGCGGTGGAGCAGCGCGCCGCGGCGCCGGGCGCGCTTGACGACCAGTCTCTTATAAACATATCCCAGCCCACGAGACCACCAGTTGCATCTCGTATTCCGTCTTATGCTTGTATATTGGGGGGGGGGGGGGGGGGGGGGG\nGAGTCGATCGAGGAGATGAAGCACGCGGAGAAGGTCATTCACCGCATCCTCTACTTCGATGCTGTCTCTTATACACATCCCGAGCCCACGAGACCACCTGTTGCATCTCGTATGCCGTCTTCTGCTTGAAAAGGGGGGGGGGGGGGGGGGG\nGGCCCGTGCAGTTCGAGATCATCTCCGAGCCCACGAGACGACCGGGTGCTTGGCGGGAGCGGCGGGGTGGTTTTATCTTCGTGGTGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGCGGCAACGCCGACACCTACCAGCTACAGCAGACGGTGCGCCGCACCATCGGCCGCTGGGTCGGCGGCCGGCTGCGCCGCCGTCCCAAGATAATCCCGGTGGTGGGGGGGGGTGGTTGGGGGGGTTGTGGGGGGGGGGGGGGGGGGGGG\nGGTAGGGCCGCTCGAGAAGCTCGCACAGCATGCGGCCGAATTCGCGGTACATGCATACGTTGACGTCGGCGGGGGGGGGGGGGGGGGGAGAGCACCGGGGGGCGCACAGGGCGGCCGGGTGGGGGTCGTGGTGGGGGGGGGGGGGGGGGGG\nGGGGAGGAGGGAGACGCGCCCGCGGGCGTGCCCGCCGCCGCGGCGATGCCCTTGAAGGCCGCTGGTGTGTCCTTCAGCGCCTGCGCCGCGAGTTCGCCGAACTGCTGCGTCAACGCGCCCCACCACTGCTGCGGGGGGGGGGGGGGGGGGG\nGTACCCGAGCCGCTCCGCGTGCCGCCGGACCTCCGACGCGTGAGGCCCGGAGCCGGTCGCTGACCAGAGGGCCAGGCCGAAGCGATGGGGAGTGGTCGCGACGAACCCGCAGCGCTGTCCGGCGGCGGGGGGGGGGGGGGGGGGGGGGGGG\nGAAGGTGTGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGTGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGCGGGGGGGGGGGGGGGCGGGGGCGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGCGCCACGCCGAGCACCGACGGCATCATCGGCACCCACACGCCGTGTGTGAACCTGTCTCTTATACACATCTCCGAGCCACGAGACCACCTGTTGCATCTCGTATGCCGTCTTCTGCTTGAAAATGGGGGGGGGGGGGGGGGGGGGGGGGG\nCTCTTCATCCGTTCCGGCGCCTGCATCCATTCCCGCGGCGCCGGTTGGCGGGGGGGGGGGGGGGGGGGGGGGGGGTGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\n</code></pre>"},{"location":"data-processing/quality-control/#fastqc-reports","title":"FastQC reports","text":"<p>Let's check the quality of the data with FastQC before quality filtering:</p> <pre><code>mkdir fastqc_untrimmed_reads\nfastqc --threads 8 /data/reads/sample_0.fq.gz -o fastqc_untrimmed_reads/\n</code></pre> <p>Question</p> <p>Open the FastqQC report <code>fastqc_untrimmed_reads//sample_0_fastqc.html</code></p>"},{"location":"data-processing/quality-control/#quality-trimming-and-filtering","title":"Quality trimming and filtering","text":"<p>Because NovaSeq and NextSeq from Illumina contain often poly-g tails, it is good to use a trimming tool that can detect poly-g tails. Fastp can do that. We keep the defaults like they are, but specify we have interleaved input data. In case you have R1 and R2 file for each sample, you need to enable adapter detection with the <code>--detect_adapter_for_pe</code> flag. Execute fastp with this command:</p> <pre><code>fastp -i /data/reads/sample_0.fq.gz \\\n      --stdout \\\n      --interleaved_in \\\n      -q 25 \\\n      --cut_front \\\n      --cut_tail \\\n      --cut_mean_quality 25 \\\n      -l 51 \\\n      --thread 16 \\\n      --trim_poly_g &gt; sample_0.trim.fastq\n</code></pre> <p>Now make a FastQC report again, to see the results of the quality filtering.</p> <pre><code>mkdir fastqc_trimmed_reads\nfastqc --threads 8 sample_0.trim.fastq -o fastqc_trimmed_reads/\n</code></pre> <p>Exercise</p> <ul> <li>Are there any adapter sequences detected?</li> <li>Take a look at the html report</li> </ul>"},{"location":"data-processing/quality-control/#alternative-trimming-with-bbduk","title":"Alternative trimming with bbduk","text":"<p>Alternative trimming with bbduk. Compare poly-g tail filtering, adapter trimming.</p> <pre><code>bbduk.sh in=/data/reads/sample_0.fq.gz  \\\n    out=sample_0.trim.bbduk.fastq.gz \\\n    interleaved=true \\\n    trimpolygright=1 \\\n    qtrim=w trimq=20 \\\n    minlength=51 \\\n    ref=/data/databases/nextera.fa.gz ktrim=r \\\n    stats=bbduk.stats \\\n    t=16\n</code></pre> <p>Question</p> <ul> <li>Create also an FastQC report for the trimming with bbduk </li> <li>What are differences between filtering with fastp and bbduk?</li> </ul>"},{"location":"data-processing/quality-control/#remove-phix-sequences","title":"Remove PhiX sequences","text":"<pre><code>bbmap.sh ref=/data/databases/phix174_ill.ref.fa.gz \\\n         in=sample_0.trim.fastq \\\n         interleaved=true \\\n         outu=sample_0.nophix.fastq.gz \\\n         outm=sample_0.phix.fastq.gz \\\n         t=4\n</code></pre> <p>Exercise</p> <ul> <li>How many PhiX sequences are detected?</li> <li>Can you confirm the sequences are PhiX?</li> <li>The input data was simulated without adding any PhiX. How could there still PhiX sequences being detected?</li> </ul>"},{"location":"data-processing/quality-control/#from-one-sample-to-many","title":"From one sample to many","text":"<p>From one sample to many</p> <p>Now do a QC for all samples. You can use a for loop for that. For example, fastp can be run like:</p> <pre><code>for file in /data/reads/*.gz; do \\\n    sample=$(basename ${file} .fq.gz);\n    fastp -i $file --stdout --interleaved_in -q 25 --cut_front --cut_tail --cut_mean_quality 25 -l 51 --thread 16 --trim_poly_g &gt; $sample.trim.fastq;\n    bbmap.sh ref=/data/databases/phix174_ill.ref.fa.gz in=$sample.trim.fastq interleaved=true outu=$sample.nophix.fastq.gz outm=$sample.phix.fastq.gz t=4;\ndone\n</code></pre>"},{"location":"data-processing/quality-control/#quickly-check-what-is-in-this-metagenome","title":"Quickly check what is in this metagenome","text":""},{"location":"data-processing/quality-control/#sendsketch","title":"Sendsketch","text":"<pre><code>sendsketch.sh --in=sample_0.nophix.fastq.gz threads=4 address=refseq\n</code></pre>"},{"location":"data-processing/quality-control/#sourmash","title":"Sourmash","text":"<p>Now try it locally, using sourmash. First create a signature for a single sample.</p> <pre><code>sourmash sketch dna -p scaled=10000,k=51 sample_0.nophix.fastq.gz -o sample_0.sig\n</code></pre> <p>Find what is in this metagenome using the <code>gather</code> command:</p> <pre><code>sourmash gather sample_0.sig /data/databases/gtdb-rs207.genomic-reps.dna.k51.lca.json.gz\n</code></pre>"},{"location":"data-processing/quality-control/#kraken","title":"Kraken","text":"<p>Compare with kraken</p> <p>First setup the database</p> <pre><code>mkdir kraken_db\ntar zxvf /data/databases/k2_standard_08gb_20220926.tar.gz -C kraken_db\n</code></pre> <p>Now run kraken2</p> <pre><code>kraken2 --db kraken_db/ --threads 16 --output sample_0.kraken --report sample_0.kraken.report --gzip-compressed --use-names sample_0.nophix.fastq.gz\n</code></pre> <p>Question</p> <p>Take a look at the <code>sample_0.kraken.report</code>. What are the differences in classification compared to sendsketch and sourmash?</p>"}]}