{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SSB57306 Advanced Omics Computational Metagenomics","text":"<p>This tutorial teaches state-of-the-art computational methods for the analysis of metagenome data, and will focus on foundational knowledge and skills in experimental design, sequencing technologies, quality control, assembly and binning.</p> <p>Software packages used are freeware.</p>"},{"location":"data-processing/assembly/","title":"Assembly","text":"<p>Learning objectives</p> <ul> <li>Being able to create an assembly with megahit</li> <li>Assess the quality of the assembly</li> </ul> <p>Now that our reads are quality trimmed and ready to go is time to start the assembly. We can use megahit:</p> <pre><code>megahit --12 sample_0.nophix.fastq.gz,sample_1.nophix.fastq.gz,sample_2.nophix.fastq.gz,sample_3.nophix.fastq.gz,sample_4.nophix.fastq.gz,sample_5.nophix.fastq.gz \\\n        -t 16 \\\n        -o megahit_assembly_meta \\\n        --presets meta-sensitive\n</code></pre> <p>This command would take around 50 minutes to complete, to speed up things we pre-assembled the data which is available in the precomputed/assembly/ folder. </p> <p>Feel free to inspect the contents of the folder by using</p> <pre><code>ls -lh precomputed/assembly/\n</code></pre> <p>You will notice the final.contigs.fa file which contains the assembly</p> <p>Now, we want to find out how well or poorly our assembly went. For this, we use quast, a tool to generate an assembly report.</p> <pre><code>quast precomputed/assembly/final.contigs.fa -o quast/\n</code></pre> <p>Then, we inspect the output from quast. </p> <pre><code>less quast/report.txt\n</code></pre> <p>Not bad! We know our metagenome is not too large and if we care about contiguity, contigs above 5kb represent most of our community</p>"},{"location":"data-processing/binning/","title":"Binning","text":"<p>Learning objectives</p> <ul> <li>Extract individual genomes from an assembly</li> </ul> <p>Next, we want to map the reads back on the assembly to see how the coverage for each contig changed across samples:</p> <pre><code>bwa mem precomputed/assembly/final.contigs.fa \\\n        reads/sample_0.fq.gz \\\n        -o bam/sample_0.bam \\\n        -t 32\n</code></pre> <p>Here, we won\u2019t run it for all samples to save time and space, you will find the bam files in the precomputed/bam/ folder. </p> <p>Question</p> <p>Feel free to inspect the content as done before, do you notice something particular?</p> <p>Now, many tools need bam files to be sorted in order to work. Therefore, we will use samtools sort to do that.</p> <pre><code>samtools sort precomputed/bam/sample_0.bam -o bam/sample_0.sorted.bam\n</code></pre> <p>The sorted bam files can also be index, so other tools can quickly extract alignments</p> <pre><code>for bam in *.sorted.bam; do samtools index $bam; done\n</code></pre> <p>We can first get an idea if the different genomes can be seperated by gc-content and coverage. With Blobtools the coverage and gc-content of the contigs can be plotted and visualy bins or blobs can be detected</p> <pre><code>wget ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump.tar.gz\ntar zxvf taxdump.tar.gz\nblobtools nodesdb --nodes db/nodes.dmp --names db/names.dmp\n\nblobtools create -i data/precomputed/assembly/final.contigs.fa -b data/precomputed/bam/sample_0.sorted.bam -b data/precomputed/bam/sample_1.sorted.bam -b data/precomputed/bam/sample_2.sorted.bam -b data/precomputed/bam/sample_3.sorted.bam -b data/precomputed/bam/sample_4.sorted.bam -b data/precomputed/bam/sample_5.sorted.bam\n\nblobtools view -i blobDB.json\nblobtools plot -i blobDB.json\n</code></pre> <p>Question</p> <p>Take a look at the <code>blobDB.json.bestsum.phylum.p8.span.100.blobplot.bam0.png</code> file. How many large genome bins can you detect? How is this for the other files?</p> <p>Now, we will create a depth table, which can be used by binning tools to identify genomic entities (contigs, here) that have similar coverage across samples.</p> <pre><code>jgi_summarize_bam_contig_depths --outputDepth depth.tsv bam/sample_0.sorted.bam bam/sample_1.sorted.bam bam/sample_2.sorted.bam bam/sample_3.sorted.bam bam/sample_4.sorted.bam bam/sample_5.sorted.bam\n</code></pre> <p>Now, we can run metabat to find our bins:</p> <pre><code>metabat2 -a depth.tsv -i precomputed/assembly/final.contigs.fa.gz -o binned_genomes/bin\n</code></pre> <p>After this is done, we will now try and figure out how good are our bins, we will use checkm. First we create a set of lineage specific markers for bacterial genomes:</p> <pre><code>checkm taxon_set domain Bacteria checkm_taxon_bacteria\n</code></pre> <p>Now, it\u2019s time to find out the completeness of your binned genomes:</p> <pre><code>checkm analyze -x fa checkm_taxon binned_genomes/ checkm_bac/\n</code></pre> <p>This created the checkm_bac/ folder with the information on our bins, to interpret the results we use checkm qa:</p> <pre><code>checkm qa checkm_taxon checkm_bac/\n</code></pre> <p>Question</p> <p>What does it mean? How can we interpret this?</p> <p>Now, we will let checkm predict the taxonomy of the bins and evaluate their completeness.</p> <pre><code>checkm lineage_wf -t 8 -x fa binned_genomes/ checkm_taxonomy/\n</code></pre> <p>This command will take some time but it will give us a detailed breakdown of each genome predicted taxonomy and completeness.</p> <p>With the bam files, a coverage file can also be created with the tool CoverM</p> <pre><code>coverm contig -b data/precomputed/bam/sample_*.sorted.bam -m mean -t 16 -o coverage.tsv\n</code></pre>"},{"location":"data-processing/cave-expedition/","title":"Cave expedition","text":"<p>Learning objectives</p> <ul> <li>Explore binning results from biofilm metagenomes in a greenhouse gas-emitting cave.</li> <li>Seek evidence identifying the primary consumers (CO2 &amp; CH4) within this extreme environment.</li> </ul> <p> (a) General overview of the cave. The dashed line in panel a marks the stable gaseous chemocline between the volcanic gases (below the chemocline) and atmospheric air (above the chemocline). (b) Detailed images of the cave biofilms. (c) A closer look on the biofilm where mark 7 shows the bare cave wall after biofilm sampling.</p> <p>Watch the \"Sulfur Cave\" video to see bubbles drifting along the invisible stream of greenhouse gases (for fun).</p> <p>Precomputed data is stored in the /data/precomputed/cave_data folder, which contains the following: 1) Co-assembly of three samples (two from biofilm and one from the laboratory), 2) Depth table and 3) Binning results</p> <p>Exercise</p> <ul> <li>Analyze the microbial bins to identify which organism is utilizing CH\u2084 (methane) for growth. What is the taxonomic classification of this organism? Investigate the presence of relevant gene clusters responsible for methane metabolism.</li> <li>Investigate whether it is common for organisms within the identified taxon to utilize CH\u2084 for growth. If not, outline the steps and analyses you would perform to confirm and demonstrate this metabolic capability.</li> <li>Perform a similar investigation to identify which organism is fixing CO\u2082 for growth. What is the taxonomy of this organism, and what genes are involved in CO\u2082 fixation?</li> </ul> Tip 1: general directions <ul> <li>Assess bin quality using CheckM to evaluate completeness and contamination levels.</li> <li>Assign taxonomies to the bins with GTDB-Tk for precise classification.</li> <li>Predict open reading frames (ORFs) using Prodigal and functionally annotate the bins through EggNOG for a deeper understanding of their metabolic capabilities.</li> </ul> Tip 2 <ul> <li>Investigate the gene annotation for methane monooxygenase and reductive TCA cycle and analyze the surrounding genomic regions to identify nearby genes and their associated protein functions.</li> <li>Calculate the abundance of each bin by integrating the depth table with the binning results (use python/R).</li> </ul>"},{"location":"data-processing/intro/","title":"Introduction","text":""},{"location":"data-processing/intro/#dataset-explanation","title":"Dataset explanation","text":"<ul> <li>In the practical you will go from raw reads to taxonomical characterization of metagenome assembled genomes</li> <li>In this mock experiment we have a small diverse synthetic rhizosphere community</li> <li>We expose the plant to fungal chitin and we expect the plant to respond to this treatment by modulating the microbial community it associates with in the rhizosphere</li> <li>Can you find out how the microbial community adapts?</li> </ul>"},{"location":"data-processing/intro/#slides","title":"Slides","text":"<p> Download the presentation (2022)  Download the presentation (2024)</p>"},{"location":"data-processing/quality-control/","title":"Quality Control","text":"<p>Learning objectives</p> <ul> <li>Understanding the FastQ format</li> <li>Interpret FastQC reports</li> <li>Create high quality reads by trimmign and filtering with FastP and BBduk</li> </ul>"},{"location":"data-processing/quality-control/#fastq-format","title":"FastQ format","text":"<p>First let's take a look at the data</p> <pre><code>zcat /data/reads/sample_0.fq.gz | head\n</code></pre> The output looks like this <pre><code>@gi|1184849861|gb|KY629563.1|-34525/1\nCTGATCAACGTGGGAATGAACATGGAGCAGAGCGGCCAGTGTCTCAACCTGTGCTCCGCCTTCATGCACTCTCGCGGCGCCTTCAAGCCCGGCGACATCGACGTGGAAATTTTCACCATGCCCACCAAGTTCGGGCGCGTCAGCACCACG\n+\nDDDGGEG*DIIGH?KKHJGHGKKKJKKJJ9JDK=JKFKKGJHIKHEIJJJKKGIHKGDE1DKA&gt;KEGBEEEBGI?BBEGHEEEFEEE9ED;FCB3BEEDCBEE5EEC'CA$E9EEEAEEEFEE?FFCEEE;$EE)DBCDEEDDECAAE'$\n@gi|1184849861|gb|KY629563.1|-34523/1\nGCCTTTGTCGGGTAAGGGGTGTGGCCCTCCTCCCGACAAGGCGGGCCACGGTTCGCCAGCGAACTAGGTCGGGGAGCTGGGAAGGAGCCGGAATCGGGTGGCCCCAATTTCGGGGAGAGGTTTGGGCGTCAGCCGCCCGGAAGCTCGTCG\n+\nDDDE2GGGIIIIIKKJKHJKEDJ=AIHKKHKJKKKKJHBIE$KJJJCEKJB=JH@JKEHKGEEEEDAJECDGC?EIFEBEDDF6FGEEDEE$FBDCEEA@EE$4EE$E??CDE?ED;CCDE1DEBE;ECC$?$$AEDA;EDD@$EEDE=F\n@gi|1184849861|gb|KY629563.1|-34521/1\nAGACCGAGCCCTTCCTTATAGTGGATTTCTCCGGTTCCGTCAACCAAATTTGCAGTAATGCGGGAGCGACGCTTCCACGAAATGGTCACTGTCCCGTCCACCTCGGAGAGCTTTACGCTTTCCGGTGTGTAGGGCTTGAGATCGATCGAT\n</code></pre> <p>There is a pattern that is repeated every 4 lines. This is the FASTQ format. It follows this structure:</p> Line Description 1 Always begins with '@' and then information about the read 2 The actual DNA sequence 3 Always begins with a '+' and sometimes the same info in line 1 4 Has a string of characters which represent the quality scores; must have same number of characters as line 2 <p>The fourth line shows the quality of the read. To make the sequence and the qaility align well, the numerical score is converted into a code where each individual character represents the numerical quality of an individual nucleotide, following this scheme:</p> <p>FASTQ quality encoding</p> <pre><code>    Quality encoding: !\"#$%&amp;'()*+,-./0123456789:;&lt;=&gt;?@ABCDEFGHIJ\n                    |         |         |         |         |\n    Quality score:    01........11........21........31........41\n</code></pre> <p>NextSeq and NovaSeq data often contains poly-g tails. Let's check if this also is the case for our simulated input</p> <pre><code>$ zcat /data/reads/sample_0.fq.gz | grep -E \"GGGGGGGGGGGGGGGGGG$\"\n</code></pre> <p>This search does not retun any hits. But often the end of the sequences contain stretches of poly-g. Below is an example how poly-g tails look like in Novaseq data.</p> Sequence data with poly-g tails <pre><code>GGCCAGGACCACGCGGTGGAGCAGCGCGCCGCGGCGCCGGGCGCGCTTGACGACCAGTCTCTTATAAACATATCCCAGCCCACGAGACCACCAGTTGCATCTCGTATTCCGTCTTATGCTTGTATATTGGGGGGGGGGGGGGGGGGGGGGG\nGAGTCGATCGAGGAGATGAAGCACGCGGAGAAGGTCATTCACCGCATCCTCTACTTCGATGCTGTCTCTTATACACATCCCGAGCCCACGAGACCACCTGTTGCATCTCGTATGCCGTCTTCTGCTTGAAAAGGGGGGGGGGGGGGGGGGG\nGGCCCGTGCAGTTCGAGATCATCTCCGAGCCCACGAGACGACCGGGTGCTTGGCGGGAGCGGCGGGGTGGTTTTATCTTCGTGGTGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGCGGCAACGCCGACACCTACCAGCTACAGCAGACGGTGCGCCGCACCATCGGCCGCTGGGTCGGCGGCCGGCTGCGCCGCCGTCCCAAGATAATCCCGGTGGTGGGGGGGGGTGGTTGGGGGGGTTGTGGGGGGGGGGGGGGGGGGGGG\nGGTAGGGCCGCTCGAGAAGCTCGCACAGCATGCGGCCGAATTCGCGGTACATGCATACGTTGACGTCGGCGGGGGGGGGGGGGGGGGGAGAGCACCGGGGGGCGCACAGGGCGGCCGGGTGGGGGTCGTGGTGGGGGGGGGGGGGGGGGGG\nGGGGAGGAGGGAGACGCGCCCGCGGGCGTGCCCGCCGCCGCGGCGATGCCCTTGAAGGCCGCTGGTGTGTCCTTCAGCGCCTGCGCCGCGAGTTCGCCGAACTGCTGCGTCAACGCGCCCCACCACTGCTGCGGGGGGGGGGGGGGGGGGG\nGTACCCGAGCCGCTCCGCGTGCCGCCGGACCTCCGACGCGTGAGGCCCGGAGCCGGTCGCTGACCAGAGGGCCAGGCCGAAGCGATGGGGAGTGGTCGCGACGAACCCGCAGCGCTGTCCGGCGGCGGGGGGGGGGGGGGGGGGGGGGGGG\nGAAGGTGTGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGTGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGCGGGGGGGGGGGGGGGCGGGGGCGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGCGCCACGCCGAGCACCGACGGCATCATCGGCACCCACACGCCGTGTGTGAACCTGTCTCTTATACACATCTCCGAGCCACGAGACCACCTGTTGCATCTCGTATGCCGTCTTCTGCTTGAAAATGGGGGGGGGGGGGGGGGGGGGGGGGG\nCTCTTCATCCGTTCCGGCGCCTGCATCCATTCCCGCGGCGCCGGTTGGCGGGGGGGGGGGGGGGGGGGGGGGGGGTGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\n</code></pre>"},{"location":"data-processing/quality-control/#fastqc-reports","title":"FastQC reports","text":"<p>Let's check the quality of the data with FastQC before quality filtering:</p> <pre><code>mkdir fastqc_untrimmed_reads\nfastqc --threads 8 /data/reads/sample_0.fq.gz -o fastqc_untrimmed_reads/\n</code></pre> <p>Question</p> <p>Open the FastqQC report <code>fastqc_untrimmed_reads//sample_0_fastqc.html</code></p>"},{"location":"data-processing/quality-control/#quality-trimming-and-filtering","title":"Quality trimming and filtering","text":"<p>Because NovaSeq and NextSeq from Illumina contain often poly-g tails, it is good to use a trimming tool that can detect poly-g tails. Fastp can do that. We keep the defaults like they are, but specify we have interleaved input data. In case you have R1 and R2 file for each sample, you need to enable adapter detection with the <code>--detect_adapter_for_pe</code> flag. Execute fastp with this command:</p> <pre><code>fastp -i /data/reads/sample_0.fq.gz \\\n      --stdout \\\n      --interleaved_in \\\n      -q 25 \\\n      --cut_front \\\n      --cut_tail \\\n      --cut_mean_quality 25 \\\n      -l 51 \\\n      --thread 16 \\\n      --trim_poly_g &gt; sample_0.trim.fastq\n</code></pre> <p>Now make a FastQC report again, to see the results of the quality filtering.</p> <pre><code>mkdir fastqc_trimmed_reads\nfastqc --threads 8 sample_0.trim.fastq -o fastqc_trimmed_reads/\n</code></pre> <p>Exercise</p> <ul> <li>Are there any adapter sequences detected?</li> <li>Take a look at the html report</li> </ul>"},{"location":"data-processing/quality-control/#alternative-trimming-with-bbduk","title":"Alternative trimming with bbduk","text":"<p>Alternative trimming with bbduk. Compare poly-g tail filtering, adapter trimming.</p> <pre><code>bbduk.sh in=/data/reads/sample_0.fq.gz  \\\n    out=sample_0.trim.bbduk.fastq.gz \\\n    interleaved=true \\\n    trimpolygright=1 \\\n    qtrim=w trimq=20 \\\n    minlength=51 \\\n    ref=/data/databases/nextera.fa.gz ktrim=r \\\n    stats=bbduk.stats \\\n    t=16\n</code></pre> <p>Question</p> <ul> <li>Create also an FastQC report for the trimming with bbduk </li> <li>What are differences between filtering with fastp and bbduk?</li> </ul>"},{"location":"data-processing/quality-control/#remove-phix-sequences","title":"Remove PhiX sequences","text":"<pre><code>bbmap.sh ref=/data/databases/phix174_ill.ref.fa.gz \\\n         in=sample_0.trim.fastq \\\n         interleaved=true \\\n         outu=sample_0.nophix.fastq.gz \\\n         outm=sample_0.phix.fastq.gz \\\n         t=4\n</code></pre> <p>Exercise</p> <ul> <li>How many PhiX sequences are detected?</li> <li>From which samples?</li> <li>Can you confirm the sequences are PhiX?</li> <li>The input data was simulated without adding any PhiX. How could there still PhiX sequences being detected?</li> </ul>"},{"location":"data-processing/quality-control/#from-one-sample-to-many","title":"From one sample to many","text":"<p>From one sample to many</p> <p>Now do a QC for all samples. You can use a for loop for that. For example, fastp can be run like:</p> <pre><code>for file in /data/reads/*.gz; do \\\n    sample=$(basename ${file} .fq.gz);\n    fastp -i $file --stdout --interleaved_in -q 25 --cut_front --cut_tail --cut_mean_quality 25 -l 51 --thread 16 --trim_poly_g &gt; $sample.trim.fastq;\n    bbmap.sh ref=/data/databases/phix174_ill.ref.fa.gz in=$sample.trim.fastq interleaved=true outu=$sample.nophix.fastq.gz outm=$sample.phix.fastq.gz t=4;\ndone\n</code></pre>"},{"location":"data-processing/quality-control/#quickly-check-what-is-in-this-metagenome","title":"Quickly check what is in this metagenome","text":""},{"location":"data-processing/quality-control/#sendsketch","title":"Sendsketch","text":"<pre><code>sendsketch.sh --in=sample_0.nophix.fastq.gz threads=4 address=refseq\n</code></pre>"},{"location":"data-processing/quality-control/#sourmash","title":"Sourmash","text":"<p>Note: The sourmash database is not included in the VM because it can't be downloaded at the moment. There is an overview of all prepared databases The 51 kmer set of representative genomes would be a good one to use when available</p> <p>Now try it locally, using sourmash. First create a signature for a sigle sample.</p> <pre><code>sourmash sketch dna -p scaled=10000,k=51 sample_0.nophix.fastq.gz -o sample_0.sig\n</code></pre> <p>Find what is in this metagenome using the <code>gather</code> command:</p> <pre><code>sourmash gather sample_0.sig /data/databases/gtdb-rs207.genomic-reps.dna.k51.lca.json.gz\n</code></pre>"},{"location":"data-processing/quality-control/#kraken","title":"Kraken","text":"<p>Compare with kraken</p> <p>First setup the database</p> <pre><code>mkdir kraken_db\ntar zxvf /data/databases/k2_standard_08gb_20220926.tar.gz -C kraken_db\n</code></pre> <p>Now run kraken2</p> <pre><code>kraken2 --db kraken_db/ --threads 16 --output sample_0.kraken --report sample_0.kraken.report --gzip-compressed --use-names sample_0.nophix.fastq.gz\n</code></pre> <p>Question</p> <p>Take a look at the <code>sample_0.kraken.report</code>. What are the differences in classification compared to sendsketch and sourmash?</p>"}]}